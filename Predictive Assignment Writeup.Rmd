---
title: "Prediction Assignment Writeup"
author: "Ignas"
date: "19/04/2020"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
## Background

Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively.The goal is to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants who were asked to perform barbell lifts correctly and incorrectly in 5 different ways.

##Task

The goal of this project is to predict the manner in which they did the exercise and build prediction model to predict 20 different test cases.

#Data preparation

Let's load necessary libraries and import the data.
```{r, message=FALSE}
library(rpart)
library(rattle)
library(caret)
library(randomForest)
set.seed(12345)

# Training and Testing data
TrainURL <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
TestURL <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"

# Download and clean the datasets
trainingdata <- read.csv(url(TrainURL), na.strings=c("NA","#DIV/0!",""))
testingdata <- read.csv(url(TestURL), na.strings=c("NA","#DIV/0!",""))
```

Dimensions of downloaded data 
```{r, comment=""}
dim(trainingdata); dim(testingdata)
```

Summary of the data (as the data is quite large, summary results are not provided).
```{r, eval=FALSE}
str(trainingdata); str(testingdata)
```

Let's delete columns with NA values and delete unnecessary columns:
```{r, comment=""}
trainingdata <-trainingdata[,colSums(is.na(trainingdata)) == 0]
testingdata <-testingdata[,colSums(is.na(testingdata)) == 0]

trainingdata <-trainingdata[,-c(1:7)]
testingdata <-testingdata[,-c(1:7)]

dim(trainingdata); dim(testingdata)
```

## Data partition

In order to be able to apply cross-validation, data will be splitted into 70% of Training data and 30 percent of Testing data.
```{r}
datapart <- createDataPartition(trainingdata$classe, p=0.7, list=FALSE)
trainingset <- trainingdata[datapart, ]
testingset <- trainingdata[-datapart, ]
dim(trainingset);dim(testingset)
```

Now we can explore variable `classe` of the training data set:
```{r}
summary(testingset$classe)
plot(testingset$classe)
```

We see that there are 5 classes where A is the most frequent and D is at least frequent. However, all of the 5 classes are distributed more or less the same. 

##Prediction

In order to generate predictions, we will use decision tree and random forest models.

###1. Decision tree model

```{r}
dtmod <- rpart(classe ~ ., data = trainingset, method = "class")
dtpred <- predict(dtmod, testingset, type = "class")
confusionMatrix(dtpred, testingset$classe)
```

Decision tree:

```{r}
fancyRpartPlot(dtmod)
```

###2. Random forest model

```{r}
rfmod <- randomForest(classe ~., data=trainingset, method="class")
rfpred <- predict(rfmod, testingset, Type="class")
confusionMatrix(rfpred, testingset$classe)
```

## Conclusion

Accuracy of decision tree model is 72.2% while using random forest is 99.3%. Out-of-sample error is 0.7% (calculated as 1-ACCURACY).

The results are as we have expected, i.e. random forest model performs better than decision tree.

## Original test set prediction

Now the results will be applied on original test date (20 observations)

```{r}
ftest <- predict(rfmod, testingdata, type = "class")
ftest

write.csv(ftest, "final_prediction.csv")
```
